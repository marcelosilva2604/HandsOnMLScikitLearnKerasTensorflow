{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Classificador de Spam Avançado - V2\n",
    "\n",
    "Este notebook implementa técnicas avançadas de Machine Learning para classificação de spam:\n",
    "- 📊 Data Augmentation para textos\n",
    "- 🧮 Feature Engineering avançado\n",
    "- 🤖 Múltiplos algoritmos (Random Forest, XGBoost, SVM, etc.)\n",
    "- 🎯 Ensemble Methods (Voting, Stacking)\n",
    "- 🔍 Otimização de hiperparâmetros\n",
    "- 📈 Validação cruzada robusta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importação de Bibliotecas e Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas essenciais\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Processamento de texto\n",
    "import re\n",
    "from html import unescape\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Algoritmos\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Configurações\n",
    "np.random.seed(42)\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails_from_folder(folder_path):\n",
    "    \"\"\"Carrega emails de uma pasta específica\"\"\"\n",
    "    emails = []\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"❌ Pasta não encontrada: {folder_path}\")\n",
    "        return emails\n",
    "    \n",
    "    files = [f for f in os.listdir(folder_path) if not f.startswith('.')]\n",
    "    \n",
    "    for filename in files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                msg = email.message_from_binary_file(f, policy=email.policy.default)\n",
    "                \n",
    "                if msg.is_multipart():\n",
    "                    body = \"\"\n",
    "                    for part in msg.walk():\n",
    "                        if part.get_content_type() == \"text/plain\":\n",
    "                            try:\n",
    "                                body += part.get_content()\n",
    "                            except:\n",
    "                                body += str(part.get_payload())\n",
    "                else:\n",
    "                    try:\n",
    "                        body = msg.get_content()\n",
    "                    except:\n",
    "                        body = str(msg.get_payload())\n",
    "                \n",
    "                # Adicionar também o subject\n",
    "                subject = msg.get('Subject', '')\n",
    "                full_text = f\"{subject} {body}\"\n",
    "                emails.append(full_text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return emails\n",
    "\n",
    "# Carregar dados\n",
    "print(\"🔄 Carregando emails...\")\n",
    "data_path = \"spam_model_data\"\n",
    "\n",
    "ham_emails = []\n",
    "spam_emails = []\n",
    "\n",
    "# HAM\n",
    "ham_emails.extend(load_emails_from_folder(os.path.join(data_path, \"easy_ham\")))\n",
    "ham_emails.extend(load_emails_from_folder(os.path.join(data_path, \"hard_ham\")))\n",
    "\n",
    "# SPAM\n",
    "spam_emails.extend(load_emails_from_folder(os.path.join(data_path, \"spam\")))\n",
    "spam_emails.extend(load_emails_from_folder(os.path.join(data_path, \"spam_2\")))\n",
    "\n",
    "print(f\"\\n📊 Dataset Original:\")\n",
    "print(f\"HAM: {len(ham_emails)}\")\n",
    "print(f\"SPAM: {len(spam_emails)}\")\n",
    "print(f\"Total: {len(ham_emails) + len(spam_emails)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Limpeza e Preprocessamento Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_clean_email(text):\n",
    "    \"\"\"Limpeza avançada de texto com preservação de features importantes\"\"\"\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return \"empty_email\"\n",
    "    \n",
    "    # Preservar indicadores de spam antes da limpeza\n",
    "    has_excessive_caps = len(re.findall(r'[A-Z]{3,}', text)) > 5\n",
    "    has_excessive_exclamation = text.count('!') > 5\n",
    "    has_money_symbols = bool(re.search(r'[$£€¥]', text))\n",
    "    \n",
    "    # Remover HTML\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # Marcar URLs como token especial\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' URL_TOKEN ', text)\n",
    "    \n",
    "    # Marcar emails\n",
    "    text = re.sub(r'\\S+@\\S+', ' EMAIL_TOKEN ', text)\n",
    "    \n",
    "    # Marcar números de telefone\n",
    "    text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', ' PHONE_TOKEN ', text)\n",
    "    \n",
    "    # Marcar valores monetários\n",
    "    text = re.sub(r'[$£€¥]\\s*\\d+(?:,\\d{3})*(?:\\.\\d{2})?', ' MONEY_TOKEN ', text)\n",
    "    \n",
    "    # Marcar percentagens\n",
    "    text = re.sub(r'\\d+\\s*%', ' PERCENT_TOKEN ', text)\n",
    "    \n",
    "    # Adicionar marcadores especiais baseados nas características detectadas\n",
    "    if has_excessive_caps:\n",
    "        text += ' EXCESSIVE_CAPS '\n",
    "    if has_excessive_exclamation:\n",
    "        text += ' EXCESSIVE_EXCLAMATION '\n",
    "    if has_money_symbols:\n",
    "        text += ' MONEY_PRESENT '\n",
    "    \n",
    "    # Limpar espaços múltiplos\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Aplicar limpeza\n",
    "print(\"🧹 Aplicando limpeza avançada...\")\n",
    "ham_clean = [advanced_clean_email(email) for email in ham_emails]\n",
    "spam_clean = [advanced_clean_email(email) for email in spam_emails]\n",
    "\n",
    "print(\"✅ Limpeza concluída!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation para Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def text_augmentation(text, num_augmented=2):\n",
    "    \"\"\"Cria variações do texto para data augmentation\"\"\"\n",
    "    augmented_texts = [text]  # Original\n",
    "    \n",
    "    words = text.split()\n",
    "    \n",
    "    for _ in range(num_augmented):\n",
    "        # Técnica 1: Synonym replacement (simplificado - trocar palavras similares)\n",
    "        aug_text = text\n",
    "        \n",
    "        # Técnica 2: Random insertion de palavras spam/ham típicas\n",
    "        spam_words = ['free', 'win', 'prize', 'offer', 'deal', 'discount', 'save', 'guarantee']\n",
    "        ham_words = ['meeting', 'project', 'report', 'update', 'schedule', 'team', 'work']\n",
    "        \n",
    "        # Técnica 3: Random swap - trocar posição de palavras\n",
    "        if len(words) > 5:\n",
    "            words_copy = words.copy()\n",
    "            for _ in range(min(3, len(words)//10)):\n",
    "                idx1, idx2 = random.sample(range(len(words_copy)), 2)\n",
    "                words_copy[idx1], words_copy[idx2] = words_copy[idx2], words_copy[idx1]\n",
    "            aug_text = ' '.join(words_copy)\n",
    "        \n",
    "        # Técnica 4: Random deletion - remover algumas palavras\n",
    "        if random.random() < 0.3 and len(words) > 10:\n",
    "            words_del = words.copy()\n",
    "            num_delete = max(1, len(words) // 20)\n",
    "            for _ in range(num_delete):\n",
    "                if len(words_del) > 5:\n",
    "                    del words_del[random.randint(0, len(words_del)-1)]\n",
    "            aug_text = ' '.join(words_del)\n",
    "        \n",
    "        augmented_texts.append(aug_text)\n",
    "    \n",
    "    return augmented_texts\n",
    "\n",
    "# Aplicar data augmentation apenas no conjunto de SPAM (minoria)\n",
    "print(\"🔄 Aplicando Data Augmentation...\")\n",
    "\n",
    "# Augmentation mais agressivo para balancear classes\n",
    "spam_augmented = []\n",
    "for email in spam_clean[:500]:  # Pegar apenas uma parte para não demorar muito\n",
    "    augmented = text_augmentation(email, num_augmented=1)\n",
    "    spam_augmented.extend(augmented)\n",
    "\n",
    "# Combinar dados\n",
    "X_all = ham_clean + spam_clean + spam_augmented\n",
    "y_all = ['ham'] * len(ham_clean) + ['spam'] * (len(spam_clean) + len(spam_augmented))\n",
    "\n",
    "print(f\"\\n📊 Dataset com Augmentation:\")\n",
    "print(f\"HAM: {y_all.count('ham')}\")\n",
    "print(f\"SPAM: {y_all.count('spam')}\")\n",
    "print(f\"Total: {len(X_all)}\")\n",
    "print(f\"Aumento de {len(spam_augmented)} amostras de SPAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extrator customizado de features para emails\"\"\"\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        features = []\n",
    "        \n",
    "        for text in X:\n",
    "            feature_dict = {}\n",
    "            \n",
    "            # Features básicas\n",
    "            feature_dict['length'] = len(text)\n",
    "            feature_dict['num_words'] = len(text.split())\n",
    "            feature_dict['num_sentences'] = text.count('.') + text.count('!') + text.count('?')\n",
    "            \n",
    "            # Features de capitalização\n",
    "            feature_dict['capital_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
    "            feature_dict['all_caps_words'] = len(re.findall(r'\\b[A-Z]+\\b', text))\n",
    "            \n",
    "            # Features de pontuação\n",
    "            feature_dict['exclamation_count'] = text.count('!')\n",
    "            feature_dict['question_count'] = text.count('?')\n",
    "            feature_dict['dollar_count'] = text.count('$')\n",
    "            \n",
    "            # Features de tokens especiais\n",
    "            feature_dict['url_count'] = text.count('URL_TOKEN')\n",
    "            feature_dict['email_count'] = text.count('EMAIL_TOKEN')\n",
    "            feature_dict['phone_count'] = text.count('PHONE_TOKEN')\n",
    "            feature_dict['money_count'] = text.count('MONEY_TOKEN')\n",
    "            feature_dict['percent_count'] = text.count('PERCENT_TOKEN')\n",
    "            \n",
    "            # Features de spam indicators\n",
    "            spam_words = ['free', 'win', 'winner', 'cash', 'prize', 'bonus', 'offer', \n",
    "                         'credit', 'guarantee', 'click', 'buy', 'discount', 'deal', 'save']\n",
    "            feature_dict['spam_word_count'] = sum(1 for word in spam_words if word.lower() in text.lower())\n",
    "            \n",
    "            # Features de complexidade\n",
    "            words = text.split()\n",
    "            if words:\n",
    "                feature_dict['avg_word_length'] = np.mean([len(word) for word in words])\n",
    "                feature_dict['max_word_length'] = max([len(word) for word in words])\n",
    "            else:\n",
    "                feature_dict['avg_word_length'] = 0\n",
    "                feature_dict['max_word_length'] = 0\n",
    "            \n",
    "            # Features de diversidade\n",
    "            unique_words = set(words)\n",
    "            feature_dict['unique_ratio'] = len(unique_words) / max(len(words), 1)\n",
    "            \n",
    "            features.append(list(feature_dict.values()))\n",
    "        \n",
    "        return np.array(features)\n",
    "\n",
    "print(\"✅ Feature Extractor criado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Divisão de Dados e Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.2, random_state=RANDOM_STATE, stratify=y_all\n",
    ")\n",
    "\n",
    "print(f\"📊 Divisão de dados:\")\n",
    "print(f\"Treino: {len(X_train)} emails\")\n",
    "print(f\"Teste: {len(X_test)} emails\")\n",
    "print(f\"\\nDistribuição no treino:\")\n",
    "print(f\"HAM: {y_train.count('ham')} ({y_train.count('ham')/len(y_train)*100:.1f}%)\")\n",
    "print(f\"SPAM: {y_train.count('spam')} ({y_train.count('spam')/len(y_train)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Criação de Pipelines com Múltiplos Algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar feature union combinando TF-IDF com features customizadas\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "\n",
    "def create_advanced_pipeline(classifier, use_custom_features=True):\n",
    "    \"\"\"Cria pipeline avançado com feature engineering\"\"\"\n",
    "    \n",
    "    if use_custom_features:\n",
    "        # Combinar TF-IDF com features customizadas\n",
    "        features = make_union(\n",
    "            TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                ngram_range=(1, 3),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                use_idf=True,\n",
    "                smooth_idf=True,\n",
    "                sublinear_tf=True\n",
    "            ),\n",
    "            Pipeline([\n",
    "                ('custom', EmailFeatureExtractor()),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "        )\n",
    "    else:\n",
    "        # Apenas TF-IDF\n",
    "        features = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 3),\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('features', features),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "print(\"✅ Função de criação de pipelines pronta!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Treinamento de Múltiplos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos para testar\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(alpha=0.1),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Treinar e avaliar cada modelo\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "print(\"🚀 Treinando modelos...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Criar pipeline (sem custom features para Naive Bayes)\n",
    "    use_custom = (name != 'Naive Bayes')\n",
    "    pipeline = create_advanced_pipeline(model, use_custom_features=use_custom)\n",
    "    \n",
    "    # Treinar\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predizer\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, pos_label='spam'),\n",
    "        'recall': recall_score(y_test, y_pred, pos_label='spam'),\n",
    "        'f1': f1_score(y_test, y_pred, pos_label='spam')\n",
    "    }\n",
    "    \n",
    "    # Salvar modelo treinado\n",
    "    trained_models[name] = pipeline\n",
    "    \n",
    "    print(f\"✅ {name}: Accuracy = {results[name]['accuracy']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparação de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\n🏆 RANKING DOS MODELOS (por F1-Score):\\n\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Visualizar resultados\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6A994E']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    values = results_df[metric].values\n",
    "    names = results_df.index\n",
    "    \n",
    "    bars = ax.bar(range(len(names)), values, color=colors)\n",
    "    ax.set_xticks(range(len(names)))\n",
    "    ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f'{metric.capitalize()} por Modelo')\n",
    "    ax.set_ylim([0.8, 1.0])\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Melhor modelo\n",
    "best_model_name = results_df.index[0]\n",
    "print(f\"\\n🥇 MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"   F1-Score: {results_df.loc[best_model_name, 'f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ensemble Methods - Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🤝 Criando Ensemble com Voting Classifier...\\n\")\n",
    "\n",
    "# Selecionar os 3 melhores modelos para o ensemble\n",
    "top_3_models = results_df.head(3).index.tolist()\n",
    "print(f\"Modelos selecionados: {top_3_models}\\n\")\n",
    "\n",
    "# Criar estimators para voting\n",
    "estimators = [(name, trained_models[name]) for name in top_3_models]\n",
    "\n",
    "# Voting Classifier - Soft voting (usa probabilidades)\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "\n",
    "# Treinar\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predizer\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "\n",
    "# Métricas\n",
    "voting_results = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_voting),\n",
    "    'precision': precision_score(y_test, y_pred_voting, pos_label='spam'),\n",
    "    'recall': recall_score(y_test, y_pred_voting, pos_label='spam'),\n",
    "    'f1': f1_score(y_test, y_pred_voting, pos_label='spam')\n",
    "}\n",
    "\n",
    "print(\"📊 Resultados do Voting Classifier:\")\n",
    "for metric, value in voting_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Adicionar aos resultados\n",
    "results['Voting Ensemble'] = voting_results\n",
    "trained_models['Voting Ensemble'] = voting_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ensemble Methods - Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📚 Criando Ensemble com Stacking Classifier...\\n\")\n",
    "\n",
    "# Base learners (usar modelos diversos)\n",
    "base_learners = [\n",
    "    ('nb', create_advanced_pipeline(MultinomialNB(alpha=0.1), use_custom_features=False)),\n",
    "    ('lr', create_advanced_pipeline(LogisticRegression(max_iter=1000, random_state=RANDOM_STATE), use_custom_features=True)),\n",
    "    ('rf', create_advanced_pipeline(RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1), use_custom_features=True))\n",
    "]\n",
    "\n",
    "# Meta learner\n",
    "meta_learner = LogisticRegression(random_state=RANDOM_STATE)\n",
    "\n",
    "# Stacking Classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,  # Usar cross-validation para treinar o meta-learner\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Treinar\n",
    "print(\"Treinando Stacking Classifier (pode demorar)...\")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predizer\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Métricas\n",
    "stacking_results = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_stacking),\n",
    "    'precision': precision_score(y_test, y_pred_stacking, pos_label='spam'),\n",
    "    'recall': recall_score(y_test, y_pred_stacking, pos_label='spam'),\n",
    "    'f1': f1_score(y_test, y_pred_stacking, pos_label='spam')\n",
    "}\n",
    "\n",
    "print(\"\\n📊 Resultados do Stacking Classifier:\")\n",
    "for metric, value in stacking_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "# Adicionar aos resultados\n",
    "results['Stacking Ensemble'] = stacking_results\n",
    "trained_models['Stacking Ensemble'] = stacking_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Otimização de Hiperparâmetros com GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Otimizando hiperparâmetros do melhor modelo...\\n\")\n",
    "\n",
    "# Vamos otimizar o Gradient Boosting\n",
    "gb_pipeline = create_advanced_pipeline(\n",
    "    GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    use_custom_features=True\n",
    ")\n",
    "\n",
    "# Grid de parâmetros\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 150, 200],\n",
    "    'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "    'classifier__max_depth': [3, 4, 5],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'features__tfidfvectorizer__max_features': [3000, 5000]\n",
    "}\n",
    "\n",
    "# GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    gb_pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Treinar (com subset para ser mais rápido)\n",
    "X_train_subset = X_train[:2000]\n",
    "y_train_subset = y_train[:2000]\n",
    "\n",
    "print(\"Executando GridSearch (pode demorar alguns minutos)...\")\n",
    "grid_search.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Melhores parâmetros\n",
    "print(f\"\\n✅ Melhores parâmetros encontrados:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Treinar modelo otimizado com dataset completo\n",
    "print(\"\\nTreinando modelo otimizado com dataset completo...\")\n",
    "optimized_model = grid_search.best_estimator_\n",
    "optimized_model.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar\n",
    "y_pred_optimized = optimized_model.predict(X_test)\n",
    "\n",
    "optimized_results = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_optimized),\n",
    "    'precision': precision_score(y_test, y_pred_optimized, pos_label='spam'),\n",
    "    'recall': recall_score(y_test, y_pred_optimized, pos_label='spam'),\n",
    "    'f1': f1_score(y_test, y_pred_optimized, pos_label='spam')\n",
    "}\n",
    "\n",
    "print(\"\\n📊 Resultados do Modelo Otimizado:\")\n",
    "for metric, value in optimized_results.items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "results['GB Optimized'] = optimized_results\n",
    "trained_models['GB Optimized'] = optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Validação Cruzada Robusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Executando Validação Cruzada Estratificada...\\n\")\n",
    "\n",
    "# Usar StratifiedKFold para manter proporção das classes\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Validar os top 3 modelos\n",
    "cv_results = {}\n",
    "\n",
    "for model_name in ['Stacking Ensemble', 'Voting Ensemble', 'GB Optimized']:\n",
    "    if model_name in trained_models:\n",
    "        print(f\"Validando {model_name}...\")\n",
    "        \n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_scores = cross_val_score(\n",
    "            model, X_train, y_train, \n",
    "            cv=skf, scoring='f1', n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        cv_results[model_name] = {\n",
    "            'mean': cv_scores.mean(),\n",
    "            'std': cv_scores.std(),\n",
    "            'scores': cv_scores\n",
    "        }\n",
    "        \n",
    "        print(f\"  F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\\n\")\n",
    "\n",
    "# Visualizar resultados da validação cruzada\n",
    "if cv_results:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    positions = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, (name, scores) in enumerate(cv_results.items()):\n",
    "        pos = [i] * len(scores['scores'])\n",
    "        positions.extend(pos)\n",
    "        plt.scatter(pos, scores['scores'], alpha=0.7, s=100)\n",
    "        plt.hlines(scores['mean'], i-0.25, i+0.25, colors='red', linewidth=2)\n",
    "        labels.append(f\"{name}\\n{scores['mean']:.3f}\")\n",
    "    \n",
    "    plt.xticks(range(len(cv_results)), labels)\n",
    "    plt.ylabel('F1-Score')\n",
    "    plt.title('Validação Cruzada - F1-Score por Fold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Análise Final e Seleção do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar todos os resultados\n",
    "final_results = pd.DataFrame(results).T\n",
    "final_results = final_results.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🏆 RESULTADOS FINAIS - TODOS OS MODELOS\")\n",
    "print(\"=\"*60)\n",
    "print(final_results.round(4))\n",
    "\n",
    "# Identificar o melhor modelo\n",
    "best_model_name = final_results.index[0]\n",
    "best_model = trained_models[best_model_name]\n",
    "best_scores = final_results.iloc[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"🥇 MODELO CAMPEÃO: {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {best_scores['accuracy']:.4f} ({best_scores['accuracy']*100:.2f}%)\")\n",
    "print(f\"Precision: {best_scores['precision']:.4f}\")\n",
    "print(f\"Recall:    {best_scores['recall']:.4f}\")\n",
    "print(f\"F1-Score:  {best_scores['f1']:.4f}\")\n",
    "\n",
    "# Comparar com baseline (primeiro modelo do spam1.ipynb)\n",
    "baseline_accuracy = 0.9721  # Do spam1.ipynb\n",
    "improvement = (best_scores['accuracy'] - baseline_accuracy) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📈 MELHORIA EM RELAÇÃO AO BASELINE (spam1.ipynb)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Baseline (TF-IDF + Dados Limpos): 97.21%\")\n",
    "print(f\"Modelo Atual: {best_scores['accuracy']*100:.2f}%\")\n",
    "if improvement > 0:\n",
    "    print(f\"🚀 MELHORIA: +{improvement:.2f} pontos percentuais!\")\n",
    "else:\n",
    "    print(f\"📊 Diferença: {improvement:.2f} pontos percentuais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Matriz de Confusão e Análise de Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer predições com o melhor modelo\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred_best, labels=['ham', 'spam'])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['HAM', 'SPAM'], \n",
    "            yticklabels=['HAM', 'SPAM'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Matriz de Confusão - {best_model_name}')\n",
    "plt.xlabel('Predição')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "# Adicionar percentuais\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm[i].sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análise de erros\n",
    "false_positives = cm[0, 1]  # HAM classificado como SPAM\n",
    "false_negatives = cm[1, 0]  # SPAM classificado como HAM\n",
    "\n",
    "print(f\"\\n📊 Análise de Erros:\")\n",
    "print(f\"Falsos Positivos (HAM → SPAM): {false_positives}\")\n",
    "print(f\"Falsos Negativos (SPAM → HAM): {false_negatives}\")\n",
    "print(f\"Taxa de Falsos Positivos: {false_positives/(false_positives + cm[0,0])*100:.2f}%\")\n",
    "print(f\"Taxa de Falsos Negativos: {false_negatives/(false_negatives + cm[1,1])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Teste com Exemplos Reais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para classificar novo email\n",
    "def classify_email(text, model=best_model):\n",
    "    \"\"\"Classifica um email como SPAM ou HAM\"\"\"\n",
    "    # Limpar texto\n",
    "    clean_text = advanced_clean_email(text)\n",
    "    \n",
    "    # Predizer\n",
    "    prediction = model.predict([clean_text])[0]\n",
    "    \n",
    "    # Probabilidades\n",
    "    try:\n",
    "        proba = model.predict_proba([clean_text])[0]\n",
    "        spam_prob = proba[1] if prediction == 'spam' else proba[0]\n",
    "        return prediction, spam_prob\n",
    "    except:\n",
    "        return prediction, None\n",
    "\n",
    "# Testar com exemplos\n",
    "test_emails = [\n",
    "    \"Congratulations! You've won $1,000,000! Click here to claim your prize NOW!\",\n",
    "    \"Hi team, please find attached the quarterly report for your review.\",\n",
    "    \"URGENT: Your account will be suspended! Verify your information immediately!\",\n",
    "    \"Meeting scheduled for tomorrow at 2 PM to discuss the project timeline.\",\n",
    "    \"Get rich quick! Make $5000 per week working from home! Limited time offer!\"\n",
    "]\n",
    "\n",
    "print(\"\\n🔍 Testando com exemplos reais:\\n\")\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    pred, prob = classify_email(email)\n",
    "    emoji = \"🚫\" if pred == 'spam' else \"✅\"\n",
    "    print(f\"Email {i}: {emoji} {pred.upper()}\")\n",
    "    if prob:\n",
    "        print(f\"  Confiança: {prob:.2%}\")\n",
    "    print(f\"  Texto: {email[:60]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Salvando o Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Salvar o melhor modelo\n",
    "model_filename = f'spam_classifier_best_{best_model_name.replace(\" \", \"_\").lower()}.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"✅ Modelo salvo como: {model_filename}\")\n",
    "print(f\"\\nPara carregar o modelo no futuro:\")\n",
    "print(f\"model = joblib.load('{model_filename}')\")\n",
    "\n",
    "# Salvar métricas\n",
    "metrics_filename = 'model_metrics.csv'\n",
    "final_results.to_csv(metrics_filename)\n",
    "print(f\"\\n📊 Métricas salvas em: {metrics_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Conclusões e Próximos Passos\n",
    "\n",
    "### 🎯 Resultados Alcançados:\n",
    "- Implementamos múltiplos algoritmos de ML\n",
    "- Aplicamos data augmentation para balancear classes\n",
    "- Criamos features customizadas avançadas\n",
    "- Implementamos ensemble methods (Voting e Stacking)\n",
    "- Otimizamos hiperparâmetros com GridSearch\n",
    "- Realizamos validação cruzada robusta\n",
    "\n",
    "### 🚀 Próximos Passos Possíveis:\n",
    "1. **Deep Learning**: Implementar redes neurais (LSTM, BERT)\n",
    "2. **Active Learning**: Melhorar com feedback humano\n",
    "3. **Online Learning**: Atualizar modelo com novos dados\n",
    "4. **Explicabilidade**: Implementar LIME/SHAP para entender decisões\n",
    "5. **Deploy**: Criar API para classificação em tempo real"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}