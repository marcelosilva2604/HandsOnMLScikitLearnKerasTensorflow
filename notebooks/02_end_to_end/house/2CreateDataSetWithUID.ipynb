{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd7814d",
   "metadata": {},
   "source": [
    "Create Test Set for Housing Data\n",
    "Following the approach from \"Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow\"\n",
    "\n",
    "This notebook demonstrates how to create a proper test set for the housing dataset\n",
    "using stratified sampling based on income categories, as recommended in the book.\n",
    "This ensures our test set is representative of the overall data distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8110d929",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\n\n\n# Read the housing dataset\nhousing = pd.read_csv('/Users/marcelosilva/Desktop/Hands-on/data/raw/housing/housing.csv')"
  },
  {
   "cell_type": "markdown",
   "id": "383a3444",
   "metadata": {},
   "source": "Creating Deterministic Dataset with Unique Identifiers\n\nWe opted to create a deterministic dataset to ensure reproducible results across different runs.\nTo achieve this, we'll use latitude and longitude coordinates to generate unique identifiers (UIDs)\nfor each housing record. This approach provides a consistent way to identify and track individual\nproperties while maintaining the spatial relationship inherent in the data.\n\nThe UIDs will be created using a hash function applied to the latitude and longitude coordinates\n(rounded to 7 decimal places), ensuring each property has a unique identifier.\n\nThe processed dataset with UIDs will be saved to:\n/Users/marcelosilva/Desktop/Hands-on/data/processed/housing"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a40e95d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def make_uid(row, existing_uids):\n",
    "    base_key = f\"{row.latitude:.7f}_{row.longitude:.7f}\"\n",
    "    uid = hashlib.sha256(base_key.encode()).hexdigest()[:16]\n",
    "    \n",
    "    # If UID already exists, add disambiguation suffix\n",
    "    counter = 1\n",
    "    original_uid = uid\n",
    "    while uid in existing_uids:\n",
    "        uid = f\"{original_uid}_{counter}\"\n",
    "        counter += 1\n",
    "    \n",
    "    existing_uids.add(uid)\n",
    "    return uid\n",
    "\n",
    "# Initialize set to track existing UIDs\n",
    "existing_uids = set()\n",
    "\n",
    "# Apply the function to create unique UIDs\n",
    "housing[\"uid\"] = housing.apply(lambda row: make_uid(row, existing_uids), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff3b14",
   "metadata": {},
   "outputs": [],
   "source": "housing.to_csv(\"/Users/marcelosilva/Desktop/Hands-on/data/processed/housing/housing_with_uid.csv\", index=False)"
  },
  {
   "cell_type": "markdown",
   "id": "5c451ae0",
   "metadata": {},
   "source": [
    "\n",
    "Feature Balance Analysis\n",
    "\n",
    "Before proceeding with the test set creation, we need to evaluate the distribution and balance of our features.\n",
    "This analysis will help us understand:\n",
    "\n",
    "- **Feature Distributions**: How each feature is distributed across the dataset\n",
    "- **Class Balance**: For categorical features, how balanced the classes are\n",
    "- **Outlier Detection**: Identify potential outliers that might affect our model\n",
    "- **Correlation Analysis**: Understand relationships between features\n",
    "\n",
    "This evaluation will inform our test set creation strategy and help ensure that our train/test split\n",
    " maintains representative distributions of all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "866018a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features in dataset: 11\n",
      "Features: ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value', 'ocean_proximity', 'uid']\n",
      "\n",
      "Numerical features (9): ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\n",
      "Categorical features (2): ['ocean_proximity', 'uid']\n",
      "\n",
      "==================================================\n",
      "NUMERICAL FEATURES ANALYSIS\n",
      "==================================================\n",
      "          longitude      latitude  housing_median_age   total_rooms  \\\n",
      "count  20640.000000  20640.000000        20640.000000  20640.000000   \n",
      "mean    -119.569704     35.631861           28.639486   2635.763081   \n",
      "std        2.003532      2.135952           12.585558   2181.615252   \n",
      "min     -124.350000     32.540000            1.000000      2.000000   \n",
      "25%     -121.800000     33.930000           18.000000   1447.750000   \n",
      "50%     -118.490000     34.260000           29.000000   2127.000000   \n",
      "75%     -118.010000     37.710000           37.000000   3148.000000   \n",
      "max     -114.310000     41.950000           52.000000  39320.000000   \n",
      "\n",
      "       total_bedrooms    population    households  median_income  \\\n",
      "count    20433.000000  20640.000000  20640.000000   20640.000000   \n",
      "mean       537.870553   1425.476744    499.539680       3.870671   \n",
      "std        421.385070   1132.462122    382.329753       1.899822   \n",
      "min          1.000000      3.000000      1.000000       0.499900   \n",
      "25%        296.000000    787.000000    280.000000       2.563400   \n",
      "50%        435.000000   1166.000000    409.000000       3.534800   \n",
      "75%        647.000000   1725.000000    605.000000       4.743250   \n",
      "max       6445.000000  35682.000000   6082.000000      15.000100   \n",
      "\n",
      "       median_house_value  \n",
      "count        20640.000000  \n",
      "mean        206855.816909  \n",
      "std         115395.615874  \n",
      "min          14999.000000  \n",
      "25%         119600.000000  \n",
      "50%         179700.000000  \n",
      "75%         264725.000000  \n",
      "max         500001.000000  \n",
      "\n",
      "==================================================\n",
      "CATEGORICAL FEATURES ANALYSIS\n",
      "==================================================\n",
      "\n",
      "ocean_proximity:\n",
      "Unique values: 5\n",
      "Most common: {'<1H OCEAN': 9136, 'INLAND': 6551, 'NEAR OCEAN': 2658}\n",
      "Missing values: 0\n",
      "\n",
      "uid:\n",
      "Unique values: 20640\n",
      "Most common: {'89880b50888801da': 1, '6e68db4ae0ba8149': 1, '6106e5a01f900f10': 1}\n",
      "Missing values: 0\n",
      "\n",
      "==================================================\n",
      "OUTLIER DETECTION (IQR Method)\n",
      "==================================================\n",
      "longitude: 0 outliers (0.00%)\n",
      "latitude: 0 outliers (0.00%)\n",
      "housing_median_age: 0 outliers (0.00%)\n",
      "total_rooms: 1287 outliers (6.24%)\n",
      "total_bedrooms: 1271 outliers (6.16%)\n",
      "population: 1196 outliers (5.79%)\n",
      "households: 1220 outliers (5.91%)\n",
      "median_income: 681 outliers (3.30%)\n",
      "median_house_value: 1071 outliers (5.19%)\n",
      "\n",
      "==================================================\n",
      "CORRELATION ANALYSIS\n",
      "==================================================\n",
      "Correlation matrix shape: (9, 9)\n",
      "\n",
      "Top correlations (absolute value > 0.5):\n",
      "longitude - latitude: -0.925\n",
      "total_rooms - total_bedrooms: 0.930\n",
      "total_rooms - population: 0.857\n",
      "total_rooms - households: 0.918\n",
      "total_bedrooms - population: 0.878\n",
      "total_bedrooms - households: 0.980\n",
      "population - households: 0.907\n",
      "median_income - median_house_value: 0.688\n",
      "\n",
      "==================================================\n",
      "MISSING VALUES ANALYSIS\n",
      "==================================================\n",
      "                Missing Values  Percentage\n",
      "total_bedrooms             207    1.002907\n"
     ]
    }
   ],
   "source": [
    "# Get all features from the dataframe\n",
    "features = housing.columns.tolist()\n",
    "print(f\"Total features in dataset: {len(features)}\")\n",
    "print(f\"Features: {features}\")\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = housing.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = housing.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# Basic statistics for numerical features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NUMERICAL FEATURES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(housing[numerical_features].describe())\n",
    "\n",
    "# Distribution analysis for categorical features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "for feature in categorical_features:\n",
    "    print(f\"\\n{feature}:\")\n",
    "    value_counts = housing[feature].value_counts()\n",
    "    print(f\"Unique values: {len(value_counts)}\")\n",
    "    print(f\"Most common: {value_counts.head(3).to_dict()}\")\n",
    "    print(f\"Missing values: {housing[feature].isnull().sum()}\")\n",
    "\n",
    "# Check for outliers in numerical features using IQR method\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OUTLIER DETECTION (IQR Method)\")\n",
    "print(\"=\"*50)\n",
    "for feature in numerical_features:\n",
    "    if feature != 'uid':  # Skip UID as it's not a meaningful feature for outlier detection\n",
    "        Q1 = housing[feature].quantile(0.25)\n",
    "        Q3 = housing[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = housing[(housing[feature] < lower_bound) | (housing[feature] > upper_bound)]\n",
    "        print(f\"{feature}: {len(outliers)} outliers ({len(outliers)/len(housing)*100:.2f}%)\")\n",
    "\n",
    "# Correlation analysis for numerical features\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "correlation_matrix = housing[numerical_features].corr()\n",
    "print(\"Correlation matrix shape:\", correlation_matrix.shape)\n",
    "print(\"\\nTop correlations (absolute value > 0.5):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.5:\n",
    "            print(f\"{correlation_matrix.columns[i]} - {correlation_matrix.columns[j]}: {corr_value:.3f}\")\n",
    "\n",
    "# Missing values analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "missing_values = housing.isnull().sum()\n",
    "missing_percentage = (missing_values / len(housing)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "print(missing_df[missing_df['Missing Values'] > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fadc3",
   "metadata": {},
   "outputs": [],
   "source": "df_com_uid = pd.read_csv('/Users/marcelosilva/Desktop/Hands-on/data/processed/housing/housing_with_uid.csv')"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee89a0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "UID UNIQUENESS CHECK\n",
      "==================================================\n",
      "Total UID count: 20640\n",
      "Unique UID count: 20640\n",
      "Are all UIDs unique? True\n",
      "✓ All UIDs are unique!\n"
     ]
    }
   ],
   "source": [
    "# Check if all UID values are unique in df_com_uid\n",
    "print(\"=\"*50)\n",
    "print(\"UID UNIQUENESS CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_uid_count = len(df_com_uid['uid'])\n",
    "unique_uid_count = df_com_uid['uid'].nunique()\n",
    "\n",
    "print(f\"Total UID count: {total_uid_count}\")\n",
    "print(f\"Unique UID count: {unique_uid_count}\")\n",
    "print(f\"Are all UIDs unique? {total_uid_count == unique_uid_count}\")\n",
    "\n",
    "if total_uid_count != unique_uid_count:\n",
    "    print(f\"Number of duplicate UIDs: {total_uid_count - unique_uid_count}\")\n",
    "    # Show duplicate UIDs if any exist\n",
    "    duplicates = df_com_uid[df_com_uid['uid'].duplicated(keep=False)]\n",
    "    print(f\"Duplicate UIDs found: {len(duplicates)} rows\")\n",
    "    print(\"Sample of duplicate UIDs:\")\n",
    "    print(duplicates[['uid']].head(10))\n",
    "else:\n",
    "    print(\"✓ All UIDs are unique!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bdd54d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "INVESTIGATING UID DUPLICATES\n",
      "==================================================\n",
      "✓ No duplicate UIDs found - this is unexpected given the previous check!\n",
      "\n",
      "==============================\n",
      "UID GENERATION INVESTIGATION\n",
      "==============================\n",
      "Sample UID generation:\n",
      "Lat: 37.880000, Lon: -122.230000\n",
      "  Expected UID: 37.880000_-122.230000\n",
      "  Actual UID:   89880b50888801da\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.860000, Lon: -122.220000\n",
      "  Expected UID: 37.860000_-122.220000\n",
      "  Actual UID:   afc03886ba0e449b\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.850000, Lon: -122.240000\n",
      "  Expected UID: 37.850000_-122.240000\n",
      "  Actual UID:   357450aa81fd6e6f\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.850000, Lon: -122.250000\n",
      "  Expected UID: 37.850000_-122.250000\n",
      "  Actual UID:   bd9be4855ebbd493\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.850000, Lon: -122.250000\n",
      "  Expected UID: 37.850000_-122.250000\n",
      "  Actual UID:   bd9be4855ebbd493_1\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.850000, Lon: -122.250000\n",
      "  Expected UID: 37.850000_-122.250000\n",
      "  Actual UID:   bd9be4855ebbd493_2\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.840000, Lon: -122.250000\n",
      "  Expected UID: 37.840000_-122.250000\n",
      "  Actual UID:   fce15d48192cb9ac\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.840000, Lon: -122.250000\n",
      "  Expected UID: 37.840000_-122.250000\n",
      "  Actual UID:   fce15d48192cb9ac_1\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.840000, Lon: -122.260000\n",
      "  Expected UID: 37.840000_-122.260000\n",
      "  Actual UID:   316f29dfd08350f8\n",
      "  Match: False\n",
      "\n",
      "Lat: 37.840000, Lon: -122.250000\n",
      "  Expected UID: 37.840000_-122.250000\n",
      "  Actual UID:   fce15d48192cb9ac_2\n",
      "  Match: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate why UIDs are not unique when they should be based on latitude and longitude\n",
    "print(\"=\"*50)\n",
    "print(\"INVESTIGATING UID DUPLICATES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for duplicate UIDs\n",
    "duplicate_uids = df_com_uid[df_com_uid['uid'].duplicated(keep=False)].sort_values('uid')\n",
    "\n",
    "if len(duplicate_uids) > 0:\n",
    "    print(f\"Found {len(duplicate_uids)} rows with duplicate UIDs\")\n",
    "    print(f\"Number of unique duplicate UID values: {duplicate_uids['uid'].nunique()}\")\n",
    "    \n",
    "    # Show first few duplicate UIDs with their lat/lon values\n",
    "    print(\"\\nFirst few duplicate UIDs with their coordinates:\")\n",
    "    for uid in duplicate_uids['uid'].unique()[:5]:\n",
    "        uid_rows = df_com_uid[df_com_uid['uid'] == uid]\n",
    "        print(f\"\\nUID: {uid}\")\n",
    "        print(uid_rows[['uid', 'latitude', 'longitude']].to_string(index=False))\n",
    "        \n",
    "        # Check if lat/lon are actually the same\n",
    "        lat_unique = uid_rows['latitude'].nunique()\n",
    "        lon_unique = uid_rows['longitude'].nunique()\n",
    "        print(f\"  Unique latitude values: {lat_unique}\")\n",
    "        print(f\"  Unique longitude values: {lon_unique}\")\n",
    "        \n",
    "        if lat_unique == 1 and lon_unique == 1:\n",
    "            print(\"  → Same coordinates, UID generation is correct\")\n",
    "        else:\n",
    "            print(\"  → Different coordinates with same UID - this is the problem!\")\n",
    "            \n",
    "    # Check if there are any rows with same lat/lon but different UIDs\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"CHECKING FOR SAME COORDINATES WITH DIFFERENT UIDS\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    # Group by lat/lon and check for multiple UIDs\n",
    "    coord_groups = df_com_uid.groupby(['latitude', 'longitude'])\n",
    "    same_coord_diff_uid = []\n",
    "    \n",
    "    for (lat, lon), group in coord_groups:\n",
    "        if group['uid'].nunique() > 1:\n",
    "            same_coord_diff_uid.append((lat, lon, group))\n",
    "    \n",
    "    if same_coord_diff_uid:\n",
    "        print(f\"Found {len(same_coord_diff_uid)} coordinate pairs with multiple UIDs!\")\n",
    "        for lat, lon, group in same_coord_diff_uid[:3]:\n",
    "            print(f\"\\nCoordinates: ({lat}, {lon})\")\n",
    "            print(f\"UIDs: {group['uid'].unique()}\")\n",
    "    else:\n",
    "        print(\"✓ No coordinate pairs with multiple UIDs found\")\n",
    "        \n",
    "else:\n",
    "    print(\"✓ No duplicate UIDs found - this is unexpected given the previous check!\")\n",
    "\n",
    "# Additional investigation: Check UID generation logic\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"UID GENERATION INVESTIGATION\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Sample a few rows to see how UID was generated\n",
    "sample_rows = df_com_uid.head(10)\n",
    "print(\"Sample UID generation:\")\n",
    "for _, row in sample_rows.iterrows():\n",
    "    expected_uid = f\"{row['latitude']:.6f}_{row['longitude']:.6f}\"\n",
    "    actual_uid = row['uid']\n",
    "    print(f\"Lat: {row['latitude']:.6f}, Lon: {row['longitude']:.6f}\")\n",
    "    print(f\"  Expected UID: {expected_uid}\")\n",
    "    print(f\"  Actual UID:   {actual_uid}\")\n",
    "    print(f\"  Match: {expected_uid == actual_uid}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}